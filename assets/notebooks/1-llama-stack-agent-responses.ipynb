{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21547149",
   "metadata": {},
   "source": [
    "# Building Agents with Llama Stack Agentic Capabilities\n",
    "\n",
    "This notebook demonstrates building **agentic systems** using Llama Stack's native capabilities for tool calling with **MCP (Model Context Protocol)** integration.\n",
    "\n",
    "For more information check [the Llama Stack Responses API docs](https://llamastack.github.io/docs/building_applications/responses_vs_agents#lls-agents-api) and the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses environment variables from `.env` file in the project root.\n",
    "Create your own `.env` file based on `.env.example`.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Llama Stack SDK                     â”‚\n",
    "â”‚         (LlamaStackClient)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                    \n",
    "         â”‚ client.responses.create()\n",
    "         â–¼                    \n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Llama Stack    â”‚   â”‚  MCP Server          â”‚\n",
    "â”‚  Responses API  â”‚   â”‚  (Server-side MCP)   â”‚\n",
    "â”‚  - vLLM Engine  â”‚â”€â”€â–¶â”‚  - Weather Service   â”‚\n",
    "â”‚  - Inference    â”‚   â”‚  - Kubernetes API    â”‚\n",
    "â”‚  - Tool Calling â”‚   â”‚  - Tool Execution    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install Llama Stack client (version must match server version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f525a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack-client==0.3.1 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (0.3.1)\n",
      "Requirement already satisfied: python-dotenv in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (4.12.0)\n",
      "Requirement already satisfied: click in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (8.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (1.9.0)\n",
      "Requirement already satisfied: fire in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (0.7.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (0.28.1)\n",
      "Requirement already satisfied: pandas in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (2.3.3)\n",
      "Requirement already satisfied: prompt-toolkit in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (3.0.52)\n",
      "Requirement already satisfied: pyaml in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (25.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (2.12.5)\n",
      "Requirement already satisfied: requests in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (2.32.5)\n",
      "Requirement already satisfied: rich in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (14.2.0)\n",
      "Requirement already satisfied: sniffio in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (3.2.0)\n",
      "Requirement already satisfied: tqdm in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from llama-stack-client==0.3.1) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from anyio<5,>=3.5.0->llama-stack-client==0.3.1) (3.11)\n",
      "Requirement already satisfied: certifi in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.3.1) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.3.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client==0.3.1) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.1) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.3.1) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from pandas->llama-stack-client==0.3.1) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from pandas->llama-stack-client==0.3.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from pandas->llama-stack-client==0.3.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from pandas->llama-stack-client==0.3.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client==0.3.1) (1.17.0)\n",
      "Requirement already satisfied: wcwidth in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from prompt-toolkit->llama-stack-client==0.3.1) (0.2.14)\n",
      "Requirement already satisfied: PyYAML in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from pyaml->llama-stack-client==0.3.1) (6.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from requests->llama-stack-client==0.3.1) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from requests->llama-stack-client==0.3.1) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from rich->llama-stack-client==0.3.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from rich->llama-stack-client==0.3.1) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/bluesman/git/byo-agentic-framework/.venv/lib64/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack-client==0.3.1) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  \"llama-stack-client==0.3.1\" \"python-dotenv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-header",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jsl1ilhed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Loading environment from: /home/bluesman/git/byo-agentic-framework/assets/notebooks/.env\n",
      "âœ… .env file FOUND and loaded\n",
      "\n",
      "ğŸ Python: /home/bluesman/git/byo-agentic-framework/.venv/bin/python\n",
      "âœ… Using virtual environment - CORRECT!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# --- Load environment variables ---\n",
    "# Automatically detect the nearest .env (walks up from current directory)\n",
    "env_path = find_dotenv(usecwd=True)\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"ğŸ“ Loading environment from: {env_path}\")\n",
    "    print(\"âœ… .env file FOUND and loaded\")\n",
    "else:\n",
    "    default_path = Path.cwd() / \".env\"\n",
    "    print(f\"ğŸ“ No .env found via find_dotenv â€” checked: {default_path}\")\n",
    "    print(\"âš ï¸  .env file NOT FOUND\")\n",
    "\n",
    "# --- Verify Python interpreter / kernel ---\n",
    "print(f\"\\nğŸ Python: {sys.executable}\")\n",
    "\n",
    "# Detect if running inside a virtual environment\n",
    "in_venv = (\n",
    "    hasattr(sys, \"real_prefix\") or\n",
    "    (getattr(sys, \"base_prefix\", sys.prefix) != sys.prefix) or\n",
    "    \"VIRTUAL_ENV\" in os.environ or\n",
    "    \"CONDA_PREFIX\" in os.environ\n",
    ")\n",
    "\n",
    "if in_venv:\n",
    "    print(\"âœ… Using virtual environment - CORRECT!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Using global Python - Consider switching kernel!\")\n",
    "    print(\"   Click 'Select Kernel' â†’ Choose 'Python (byo-agentic-framework)')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Create client using Llama Stack's base URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13094af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Llama Stack Configuration:\n",
      "   Base URL: http://localhost:8321/\n",
      "   OpenAI Endpoint: http://localhost:8321/v1\n",
      "âœ… Llama Stack client created\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Get configuration from environment variables\n",
    "LLAMA_STACK_BASE_URL = os.getenv(\"LLAMA_STACK_BASE_URL\")\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "\n",
    "print(\"ğŸŒ Llama Stack Configuration:\")\n",
    "print(f\"   Base URL: {LLAMA_STACK_BASE_URL}\")\n",
    "print(f\"   OpenAI Endpoint: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "\n",
    "# Create client using the base URL\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_BASE_URL)\n",
    "print(\"âœ… Llama Stack client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-header",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility for pretty-printing response objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48255cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date\n",
    "\n",
    "def pretty_print(obj) -> None:\n",
    "    \"\"\"\n",
    "    Print object as formatted JSON.\n",
    "    Handles nested objects and lists.\n",
    "    \"\"\"\n",
    "    def recursive_serializer(o):\n",
    "        if hasattr(o, '__dict__'):\n",
    "            return o.__dict__\n",
    "        if isinstance(o, date):\n",
    "            return o.isoformat()\n",
    "        raise TypeError(f\"Object of type {o.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "    data_to_serialize = obj.__dict__ if hasattr(obj, \"__dict__\") else obj\n",
    "    print(json.dumps(data_to_serialize, indent=2, default=recursive_serializer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-header",
   "metadata": {},
   "source": [
    "## Example 1: Agent with Tools Available (No Tool Usage)\n",
    "\n",
    "Test the agent with a basic question. The agent has tools available but determines they're not needed for this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "MCP_WEATHER_SERVER_URL = os.getenv(\"MCP_CUSTOMER_SERVER_URL\")\n",
    "\n",
    "EXAMPLE_PROMPT = \"Give me list of customers of Fantaco company\"\n",
    "\n",
    "print(f\"ğŸ¤– Model: {INFERENCE_MODEL}\")\n",
    "print(f\"ğŸ’¬ Prompt: {EXAMPLE_PROMPT}\\n\")\n",
    "\n",
    "# Use Llama Stack's Responses API with MCP tools available\n",
    "responses = client.responses.create(\n",
    "    model=INFERENCE_MODEL,\n",
    "    input=EXAMPLE_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(responses.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PROMPT = \"What is the weather in Boston? Use the getforecast tool.\"\n",
    "\n",
    "print(f\"ğŸ¤– Model: {INFERENCE_MODEL}\")\n",
    "print(f\"ğŸ’¬ Prompt: {EXAMPLE_PROMPT}\\n\")\n",
    "\n",
    "# Use Llama Stack's Responses API with MCP tools available\n",
    "responses = client.responses.create(\n",
    "    model=INFERENCE_MODEL,\n",
    "    input=EXAMPLE_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(responses.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2-header",
   "metadata": {},
   "source": [
    "## Example 2: Agentic Tool Execution\n",
    "\n",
    "Test the agent with a weather query. The agent will:\n",
    "1. **Discover** available MCP tools\n",
    "2. **Decide** which tool to use (getforecast)\n",
    "3. **Execute** the tool call with appropriate arguments\n",
    "4. **Synthesize** the tool results into a natural language response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75f3d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Model: ollama/llama3.2:3b\n",
      "ğŸŒ¦ï¸  MCP Server: https://mcp-customer-route-nvinto-dev.apps.rm3.7wse.p1.openshiftapps.com/mcp\n",
      "ğŸ’¬ Prompt: Search customer with name Anabela Domingues\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Agent Execution Trace:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Output 1] Type: mcp_list_tools\n",
      "  Server: customer\n",
      "  Tools available: ['search_customers', 'get_customer']\n",
      "\n",
      "[Output 2] Type: mcp_call\n",
      "  Tool called: search_customers\n",
      "  Arguments: {\"contact_name\":\"Anabela Domingues\"}\n",
      "  Result: {\"results\":[{\"customerId\":\"TRADH\",\"companyName\":\"TradiÃ§Ã£o Hipermercados\",\"contactName\":\"Anabela Domingues\",\"contactTitle\":\"Sales Representative\",\"address\":\"Av. InÃªs de Castro, 414\",\"city\":\"Sao Paulo\",...\n",
      "\n",
      "[Output 3] Type: message\n",
      "  Role: assistant\n",
      "  Content: The customer Anabela Domingues was found in the system database. Here is a summary of the information we have:\n",
      "\n",
      "* Name: Anabela Domingues\n",
      "* Company: TradiÃ§Ã£o Hipermercados\n",
      "* Contact Position: Sales Representative\n",
      "* Address: Av. InÃªs de Castro, 414, Sao Paulo, SP, 05634-030, Brazil\n",
      "* Phone Number: (11) 555-2167\n",
      "* Fax Number: (11) 555-2168\n",
      "* Email: anabeladomingues@example.com\n",
      "\n",
      "Would you like to know anything else about the customer or perform further actions?\n",
      "\n",
      "================================================================================\n",
      "FINAL RESPONSE:\n",
      "================================================================================\n",
      "The customer Anabela Domingues was found in the system database. Here is a summary of the information we have:\n",
      "\n",
      "* Name: Anabela Domingues\n",
      "* Company: TradiÃ§Ã£o Hipermercados\n",
      "* Contact Position: Sales Representative\n",
      "* Address: Av. InÃªs de Castro, 414, Sao Paulo, SP, 05634-030, Brazil\n",
      "* Phone Number: (11) 555-2167\n",
      "* Fax Number: (11) 555-2168\n",
      "* Email: anabeladomingues@example.com\n",
      "\n",
      "Would you like to know anything else about the customer or perform further actions?\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "MCP_CUSTOMER_SERVER_URL = os.getenv(\"MCP_CUSTOMER_SERVER_URL\")\n",
    "\n",
    "EXAMPLE_PROMPT = \"Search customer with name Anabela Domingues\"\n",
    "\n",
    "print(f\"ğŸ¤– Model: {INFERENCE_MODEL}\")\n",
    "print(f\"ğŸŒ¦ï¸  MCP Server: {MCP_CUSTOMER_SERVER_URL}\")\n",
    "print(f\"ğŸ’¬ Prompt: {EXAMPLE_PROMPT}\\n\")\n",
    "\n",
    "# Use Llama Stack's Responses API\n",
    "agent_responses = client.responses.create(\n",
    "    model=INFERENCE_MODEL,\n",
    "    input=EXAMPLE_PROMPT,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",  # Server-side MCP\n",
    "            \"server_url\": MCP_CUSTOMER_SERVER_URL,\n",
    "            \"server_label\": \"customer\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Display execution trace\n",
    "print(\"ğŸ¤– Agent Execution Trace:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, output in enumerate(agent_responses.output):\n",
    "    print(f\"\\n[Output {i + 1}] Type: {output.type}\")\n",
    "    \n",
    "    if output.type == \"mcp_list_tools\":\n",
    "        print(f\"  Server: {output.server_label}\")\n",
    "        print(f\"  Tools available: {[t.name for t in output.tools]}\")\n",
    "    \n",
    "    elif output.type == \"mcp_call\":\n",
    "        print(f\"  Tool called: {output.name}\")\n",
    "        print(f\"  Arguments: {output.arguments}\")\n",
    "        print(f\"  Result: {output.output[:200]}...\")  # Truncate long output\n",
    "        if output.error:\n",
    "            print(f\"  Error: {output.error}\")\n",
    "    \n",
    "    elif output.type == \"message\":\n",
    "        print(f\"  Role: {output.role}\")\n",
    "        if hasattr(output.content[0], 'text'):\n",
    "            print(f\"  Content: {output.content[0].text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(agent_responses.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847005c6",
   "metadata": {},
   "source": [
    "## Example 3: Agentic Tool Execution - Kubernetes MCP Server\n",
    "\n",
    "Test the agent with a OpenShift query. The agent will:\n",
    "1. **Discover** available MCP tools\n",
    "2. **Decide** which tool to use (getforecast)\n",
    "3. **Execute** the tool call with appropriate arguments\n",
    "4. **Synthesize** the tool results into a natural language response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86902c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "MCP_OCP_SERVER_URL = os.getenv(\"MCP_OCP_SERVER_URL\")\n",
    "\n",
    "EXAMPLE_PROMPT = \"Give me a list of the pods in the ai-bu-shared. Use the the best tool for the available tools.\"\n",
    "\n",
    "print(f\"ğŸ¤– Model: {INFERENCE_MODEL}\")\n",
    "print(f\"ğŸŒ¦ï¸  MCP Server: {MCP_OCP_SERVER_URL}\")\n",
    "print(f\"ğŸ’¬ Prompt: {EXAMPLE_PROMPT}\\n\")\n",
    "\n",
    "# Use Llama Stack's Responses API\n",
    "agent_responses = client.responses.create(\n",
    "    model=INFERENCE_MODEL,\n",
    "    input=EXAMPLE_PROMPT,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",  # Server-side MCP\n",
    "            \"server_url\": MCP_OCP_SERVER_URL,\n",
    "            \"server_label\": \"k8s\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Display execution trace\n",
    "print(\"ğŸ¤– Agent Execution Trace:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, output in enumerate(agent_responses.output):\n",
    "    print(f\"\\n[Output {i + 1}] Type: {output.type}\")\n",
    "    \n",
    "    if output.type == \"mcp_list_tools\":\n",
    "        print(f\"  Server: {output.server_label}\")\n",
    "        print(f\"  Tools available: {[t.name for t in output.tools]}\")\n",
    "    \n",
    "    elif output.type == \"mcp_call\":\n",
    "        print(f\"  Tool called: {output.name}\")\n",
    "        print(f\"  Arguments: {output.arguments}\")\n",
    "        print(f\"  Result: {output.output[:200]}...\")  # Truncate long output\n",
    "        if output.error:\n",
    "            print(f\"  Error: {output.error}\")\n",
    "    \n",
    "    elif output.type == \"message\":\n",
    "        print(f\"  Role: {output.role}\")\n",
    "        if hasattr(output.content[0], 'text'):\n",
    "            print(f\"  Content: {output.content[0].text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(agent_responses.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
