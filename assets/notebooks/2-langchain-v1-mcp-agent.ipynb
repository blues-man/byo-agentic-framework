{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents and Llama Stack with MCP Tools\n",
    "\n",
    "This notebook demonstrates **LangChain 1.0** integration with **Llama Stack** responses API using proper **MCP (Model Context Protocol) adapters** for client-side tool execution.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses environment variables from `.env` file in the project root.\n",
    "Create your own `.env` file based on `.env.example`.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         LangChain 1.0 Agent                 ‚îÇ\n",
    "‚îÇ         (create_agent)                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                    ‚îÇ\n",
    "         ‚îÇ Model Calls        ‚îÇ Tool Execution\n",
    "         ‚ñº                    ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Llama Stack    ‚îÇ   ‚îÇ  MCP Client          ‚îÇ\n",
    "‚îÇ  (OpenAI API)   ‚îÇ   ‚îÇ  (langchain-mcp)     ‚îÇ\n",
    "‚îÇ  - vLLM         ‚îÇ   ‚îÇ  - SSE Transport     ‚îÇ\n",
    "‚îÇ  - Llama 3.2    ‚îÇ   ‚îÇ  - Tool Adapters     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                             ‚îÇ\n",
    "                             ‚ñº\n",
    "                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                      ‚îÇ MCP Server   ‚îÇ\n",
    "                      ‚îÇ (Weather)    ‚îÇ\n",
    "                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install LangChain 1.0 and MCP adapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q \"langchain>=1.0\" \"langchain-openai>=0.3.32\" \"langchain-core>=0.3.75\" \"langchain-mcp-adapters>=0.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain 1.0 imports\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# MCP Adapters for client-side tool execution\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Load environment variables from project root\n",
    "project_root = Path.cwd().parent.parent if 'assets/notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "env_path = project_root / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "print(f\"üìÅ Loading environment from: {env_path}\")\n",
    "print(f\"‚úÖ .env file {'found' if env_path.exists() else 'NOT FOUND'}\")\n",
    "print(\"‚úÖ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Connect to Llama Stack's OpenAI-compatible endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Llama Stack Configuration ===\n",
    "# Load from environment variables\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "API_KEY = os.getenv(\"API_KEY\", \"fake\")\n",
    "\n",
    "print(\"üåê Llama Stack Configuration:\")\n",
    "print(f\"   Endpoint using Responses API: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "\n",
    "# Create ChatOpenAI client pointing to Llama Stack\n",
    "llm = ChatOpenAI(\n",
    "    model=INFERENCE_MODEL,\n",
    "    api_key=API_KEY,\n",
    "    base_url=LLAMA_STACK_OPENAI_ENDPOINT,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"\\nüß™ Testing connectivity...\")\n",
    "try:\n",
    "    response = llm.invoke(\"Say 'Connection successful' if you can read this.\")\n",
    "    print(f\"üì• LLM Response: {response.content}\")\n",
    "    print(f\"üì• LLM Response metadata: {response.response_metadata}\")\n",
    "    print(\"‚úÖ Llama Stack connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up MCP Client and Tools\n",
    "\n",
    "Configure the MCP client to connect to the weather service using SSE transport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MCP Client Configuration ===\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Load MCP server URL from environment\n",
    "MCP_WEATHER_SERVER_URL = os.getenv(\"MCP_WEATHER_SERVER_URL\")\n",
    "\n",
    "print(\"üõ†Ô∏è  Configuring MCP client...\\n\")\n",
    "print(f\"üå¶Ô∏è  MCP Server URL: {MCP_WEATHER_SERVER_URL}\")\n",
    "\n",
    "# Step 1: Create the MCP client\n",
    "client = MultiServerMCPClient({\n",
    "    \"weather\": {\n",
    "        \"transport\": \"sse\",\n",
    "        \"url\": MCP_WEATHER_SERVER_URL,\n",
    "    }\n",
    "})\n",
    "\n",
    "# Step 2: Get tools from the MCP server (async -> await)\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# Step 3: Show what we got\n",
    "print(f\"\\nüì¶ Loaded {len(tools)} tools from MCP server:\")\n",
    "for tool in tools:\n",
    "    print(f\"   - {tool.name}: {tool.description}\")\n",
    "\n",
    "print(\"\\n‚úÖ MCP tools configured successfully!\")\n",
    "\n",
    "# We keep 'client' alive so tools can work later\n",
    "mcp_client = client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LangChain Agent\n",
    "\n",
    "Use the new `create_agent` API from LangChain 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create Agent using LangChain v1.0 ===\n",
    "print(\"ü§ñ Creating LangChain agent...\\n\")\n",
    "\n",
    "# Create agent with proper configuration\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"\"\"\n",
    "You are a helpful weather assistant powered by Llama Stack.\n",
    "\n",
    "You have access to weather tools that can retrieve current weather information.\n",
    "When a user asks about weather, use the available tools to get accurate data.\n",
    "\n",
    "Always:\n",
    "- Be concise and friendly\n",
    "- Use tools when needed to get real data\n",
    "- Provide clear, actionable information\n",
    "\"\"\".strip(),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent created successfully!\")\n",
    "print(\"\\nüìä Agent Configuration:\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "print(f\"   Tools: {len(tools)} MCP tools available\")\n",
    "print(f\"   Framework: LangChain 1.0 (create_agent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Let's test the agent with a weather query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test Agent ===\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Testing LangChain 1.0 Agent with MCP Tools\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Define async function to run agent\n",
    "async def run_agent_query(query: str):\n",
    "    \"\"\"\n",
    "    Run agent query and display results.\n",
    "    \"\"\"\n",
    "    print(f\"üë§ User: {query}\\n\")\n",
    "    \n",
    "    # Invoke agent\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"ü§ñ Agent Execution Trace:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nüë§ Human: {message.content}\")\n",
    "        \n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"\\nü§ñ AI (Tool Call):\")\n",
    "                for tool_call in message.tool_calls:\n",
    "                    print(f\"   Tool Calling: {tool_call['name']}\")\n",
    "                    print(f\"   Args: {tool_call['args']}\")\n",
    "            else:\n",
    "                print(f\"\\nü§ñ AI: {message.content}\")\n",
    "        \n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"\\nüõ†Ô∏è  Tool Result: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Query completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run test query\n",
    "result = await run_agent_query(\"What is the weather in Miami?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byo-agentic-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
